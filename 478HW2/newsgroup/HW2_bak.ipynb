{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## CSC 478 Programming Machine Learning Applications\n",
    "Siravich Khongrod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Create your own KNN classifier function. Your classifier should allow as input the training data matrix, the training labels, the instance to be classified, the value of K, and should return the predicted class for the instance and the top K neighbors. Your classifier should work with Euclidean distance as well as Cosine Similarity. You may create two separate classifiers, or add this capability as a parameter for the classifier function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_search(dataPt, dataMat, K, measure):\n",
    "    if measure == 0: # euclidean distances\n",
    "        dists = np.sqrt(((dataMat - dataPt)**2).sum(axis=1))\n",
    "    elif measure == 1: # cosine similarity\n",
    "        D_norm = np.array([np.linalg.norm(D[i]) for i in range(len(D))])\n",
    "        x_norm = np.linalg.norm(x)\n",
    "        sims = np.dot(D,x)/(D_norm * x_norm)\n",
    "        dists = 1 - sims\n",
    "\n",
    "    idx = np.argsort(dists) # sorting\n",
    "    # return the indexes of K nearest neighbors\n",
    "    return idx[:K], sorted(dists)[:K]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Create a function to compute the classification accuracy over the test data set (ratio of correct predictions to the number of test instances). This function will call the classifier function in part a on all the test instances and in each case compares the actual test class label to the predicted class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. trainMatrixModified.txt: the term-document frequency matrix for the training documents. Each row of this matrix corresponds to one the terms and each column corresponds to one the documents and the (i,j)th element of the matrix shows the frequency of the ith term in the jth document. This matrix contains 5500 rows and 800 columns.\n",
    "\n",
    "2. testMatrixModified.txt: the term-document frequency for the test documents. The matrix contains 5500 rows and 200 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# term-doc\n",
    "trainDF = pd.read_csv('trainMatrixModified.txt',sep='\\t',header=None)\n",
    "\n",
    "trainDT = trainDF.T\n",
    "# trainDT.head()\n",
    "\n",
    "testDF = pd.read_csv('testMatrixModified.txt',sep='\\t',header=None)\n",
    "testDT = testDF.T\n",
    "# testDT.head()\n",
    "\n",
    "trainClassDF = pd.read_csv('trainClasses.txt',sep='\\t',header=None)\n",
    "testClassDF = pd.read_csv('testClasses.txt',sep='\\t',header=None)\n",
    "\n",
    "\n",
    "testSet = testDT\n",
    "testSet = np.array(testSet)\n",
    "neigh_idx, distances = knn_search(testDT.iloc[1],trainDT,3,0)\n",
    "\n",
    "trainClassDF.iloc[neigh_idx][1]\n",
    "# knn_search(testDT.iloc[0],trainDT,3,0)\n",
    "\n",
    "neigh_labels = trainClassDF.iloc[neigh_idx][1]\n",
    "\n",
    "neigh_labels.mode()\n",
    "\n",
    "# from collections import Counter\n",
    "# print Counter(neigh_labels)\n",
    "\n",
    "# c = Counter(neigh_labels).most_common(1)\n",
    "\n",
    "# c[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classify (dataPt, dataMat, K, measure):\n",
    "    neigh_idx, distances = knn_search(dataPt, dataMat, K, measure)\n",
    "    neigh_labels = trainClassDF.iloc[neigh_idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in testDT:\n",
    "    neigh_idx, distances = knn_search(testDT.iloc[1],trainDT,3,0)\n",
    "    neigh_labels = trainClassDF.iloc[neigh_idx][1]\n",
    "    Counter(neigh_labels).most_common(1)\n",
    "\n",
    "\n",
    "# trainDT.iloc[798]\n",
    "# trainClassDF = np.array(trainClassDF)\n",
    "# trainClassDF[neigh_idx]\n",
    "# trainClassDF[1]\n",
    "# range(testSet)\n",
    "# testSet\n",
    "\n",
    "# len(testClassDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "C. Run your accuracy function on a range of values for K in order to compare accuracy values for different numbers of neighbors. Do this both using Euclidean Distance as well as Cosine similarity measure. [For example, you can try evaluating your classifiers on a range of values of K from 1 through 20 and present the results as a table or a graph]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Using Python, modify the training and test data sets so that term weights are converted to TFxIDF weights (instead of raw term frequencies). [See class notes on Text Categorization]. Then, rerun your evaluation on the range of K values (as above) and compare the results to the results without using TFxIDF weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Create a new classifier based on the Rocchio Method adapted for text categorization [See class notes on Text Categorization]. You should separate the training function from the classifiation function. The training part for the classifier can be implemented as a function that takes as input the training data matrix and the training labels, returning the prototype vectors for each class. The classification part can be implemented as another function that would take as input the prototypes returned from the training function and the instance to be classified. This function should measure Cosine similarity of the test instance to each prototype vector. Your output should indicate the predicted class for the test instance and the similarity values of the instance to each of the category prototypes. Finally, compute the classification accuracy using the test instances and compare your results to the best KNN approach you tried earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
