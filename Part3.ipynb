{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\S_kho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. First remove stop words, and punctuation, and apply Porter's stemming algorithm to the 3 documents (Note: You can write your program or use the online stemming application for this purpose).  Show the resulting documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glimps', 'index', 'queri', 'system', 'allow', 'search', 'file', 'system', 'document', 'collect', 'quickli', 'glimps', 'default', 'search', 'engin', 'larger', 'inform', 'retriev', 'system', 'also', 'use', 'part', 'web', 'base', 'search', 'engin']\n",
      "\n",
      "\n",
      "['main', 'process', 'retriev', 'system', 'document', 'index', 'queri', 'process', 'queri', 'evalu', 'relev', 'feedback', 'among', 'effici', 'updat', 'index', 'critic', 'larg', 'scale', 'system']\n",
      "\n",
      "\n",
      "['cluster', 'creat', 'short', 'snippet', 'document', 'retriev', 'web', 'search', 'engin', 'good', 'cluster', 'creat', 'full', 'text', 'web', 'document']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = ['Glimpse is an indexing and query system that allows for search through a file system or document collection quickly. Glimpse is the default search engine in a larger information retrieval system. It has also been used as part of some web based search engines.'\n",
    "        ,'The main processes in an retrieval system are document indexing, query processing, query evaluation and relevance feedback. Among these, efficient updating of the index is critical in large scale systems.'\n",
    "        ,'Clusters are created from short snippets of documents retrieved by web search engines which are as good as clusters created from the full text of web documents.']\n",
    "\n",
    "for i in range(0,len(docs)):\n",
    "    docs[i] = docs[i].lower()\n",
    "#   print(docs[i].lower()+'\\n')\n",
    "docs\n",
    "\n",
    "# tokens = word_tokenize(text)\n",
    "# tokens = [w for w in tokens if w.isalpha()] # filter tokens that contain non-alphabetic character(s)\n",
    "# print (tokens)\n",
    "\n",
    "docs_token = docs\n",
    "# docs_token[0] = word_tokenize(docs[0])\n",
    "docs_token\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "for i in range(0,3):\n",
    "    docs_token[i] = word_tokenize(docs[i])\n",
    "    docs_token[i] = [w for w in docs_token[i] if w not in stopwords.words('english')] # filter English stopwords\n",
    "    docs_token[i] = [porter.stem(tok) for tok in docs_token[i]] # apply stemmer\n",
    "    docs_token[i] = [w for w in docs_token[i] if w.isalpha()] # filter tokens that contain non-alphabetic character(s)\n",
    "# docs_token\n",
    "# docs_token = [w for w in docs_token if w not in stopwords.words('english')] # filter English stopwords\n",
    "\n",
    "for doc_t in docs_token:\n",
    "    print(doc_t)\n",
    "    print('\\n')\n",
    "# print (docs_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Create an inverted index of the three documents, including the dictionary and the postings. The dictionary should also contain (for each term) statistics such as total number of occurrences in the collection and the document frequency. The postings for each term should contain the document ids and the term frequencies (depict multiple postings for a term as a linked list, similar to Figure 1.3 in the IR Book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://spapas.github.io/2016/04/27/python-nested-list-comprehensions/\n",
    "# non_flat = [ [1,2,3], [4,5,6], [7,8] ]\n",
    "# >>> [y for x in non_flat for y in x]\n",
    "# [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# for x in non_flat:\n",
    "#     for y in x:\n",
    "#         y\n",
    "\n",
    "# from collections import Counter\n",
    "# word_count_dict = Counter(word for doc_t in docs_token for word in doc_t)\n",
    "# word_count_dict\n",
    "\n",
    "word_count_dict = nltk.FreqDist(word for doc_t in docs_token for word in doc_t)\n",
    "# word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['glimps', 'index', 'queri', 'system', 'allow', 'search', 'file', 'document', 'collect', 'quickli', 'default', 'engin', 'larger', 'inform', 'retriev', 'also', 'use', 'part', 'web', 'base', 'main', 'process', 'evalu', 'relev', 'feedback', 'among', 'effici', 'updat', 'critic', 'larg', 'scale', 'cluster', 'creat', 'short', 'snippet', 'good', 'full', 'text'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cluster': 2, 'glimps': 0, 'main': 1, 'process': 'doc1'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_idx = {}\n",
    "inv_idx[\"process\"] = \"doc1\"\n",
    "inv_idx\n",
    "docs_token[0][0]\n",
    "inv_idx[docs_token[0][0]] = 1\n",
    "inv_idx\n",
    "\n",
    "# inv_idx = {}\n",
    "# for doc_t in docs_token:\n",
    "#     for token in doc_t:\n",
    "#         inv_idx[]\n",
    "        \n",
    "for i in range(0,len(docs_token)):\n",
    "    for token in docs_token[i]:\n",
    "        inv_idx[token] = i\n",
    "        break\n",
    "        \n",
    "inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allow: [0]\n",
      "also: [0]\n",
      "among: [1]\n",
      "base: [0]\n",
      "cluster: [2]\n",
      "collect: [0]\n",
      "creat: [2]\n",
      "critic: [1]\n",
      "default: [0]\n",
      "document: [0, 1, 2]\n",
      "effici: [1]\n",
      "engin: [0, 2]\n",
      "evalu: [1]\n",
      "feedback: [1]\n",
      "file: [0]\n",
      "full: [2]\n",
      "glimps: [0]\n",
      "good: [2]\n",
      "index: [0, 1]\n",
      "inform: [0]\n",
      "larg: [1]\n",
      "larger: [0]\n",
      "main: [1]\n",
      "part: [0]\n",
      "process: [1]\n",
      "queri: [0, 1]\n",
      "quickli: [0]\n",
      "relev: [1]\n",
      "retriev: [0, 1, 2]\n",
      "scale: [1]\n",
      "search: [0, 2]\n",
      "short: [2]\n",
      "snippet: [2]\n",
      "system: [0, 1]\n",
      "text: [2]\n",
      "updat: [1]\n",
      "use: [0]\n",
      "web: [0, 2]\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/28019543/inverted-index-given-a-list-of-document-tokens-using-python\n",
    "# https://codereview.stackexchange.com/questions/188918/creating-an-inverted-index-in-python\n",
    "\n",
    "# docs_token = [['glimps', 'index', 'queri', 'system', 'allow', 'search', 'file', 'system', 'document', 'collect', 'quickli', 'glimps', 'default', 'search', 'engin', 'larger', 'inform', 'retriev', 'system', 'also', 'use', 'part', 'web', 'base', 'search', 'engin']\n",
    "# ,['main', 'process', 'retriev', 'system', 'document', 'index', 'queri', 'process', 'queri', 'evalu', 'relev', 'feedback', 'among', 'effici', 'updat', 'index', 'critic', 'larg', 'scale', 'system']\n",
    "# ,['cluster', 'creat', 'short', 'snippet', 'document', 'retriev', 'web', 'search', 'engin', 'good', 'cluster', 'creat', 'full', 'text', 'web', 'document']\n",
    "# ]\n",
    "from collections import defaultdict\n",
    "index = defaultdict(list)\n",
    "\n",
    "for i in range(0,len(docs_token)):\n",
    "    for token in docs_token[i]:\n",
    "        if i not in index[token]:\n",
    "            index[token].append(i)\n",
    "            \n",
    "for key in sorted(index.keys()):\n",
    "    print(key+\": \"+str(index[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What are the search results for the following Boolean queries (in each case explain how you obtained them from the inverted index):\n",
    "index AND query\n",
    "index OR query\n",
    "index AND (NOT query)\n",
    "(search AND query) OR (search AND retrieve)\n",
    "(index OR cluster) AND (web OR system)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index AND query\n",
      "set()\n",
      "index AND queri\n",
      "{0, 1}\n",
      "index OR query\n",
      "{0, 1}\n",
      "index AND (NOT query)\n",
      "set()\n",
      "{0, 1}\n",
      "(search AND query) OR (search AND retrieve)\n",
      "{0, 2}\n",
      "(index OR cluster) AND (web OR system)\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "def search_and(index,term1,term2):\n",
    "    return set(index[term1]) & set(index[term2])\n",
    "\n",
    "def search_or(index,term1,term2):\n",
    "    return set(index[term1]) | set(index[term2])\n",
    "\n",
    "# index AND query\n",
    "print(\"index AND query\")\n",
    "print(search_and(index,'index','query'))\n",
    "print(\"index AND queri\")\n",
    "print(search_and(index,'index','queri'))\n",
    "\n",
    "# index OR query\n",
    "print(\"index OR query\")\n",
    "print(search_or(index,'index','queri'))\n",
    "\n",
    "# index AND (NOT query)\n",
    "print(\"index AND (NOT query)\")\n",
    "# do you mean queri?\n",
    "# index AND (NOT queri) <- stemmed query\n",
    "print(set(index['index']) - set(index['queri']))\n",
    "# index AND (NOT query)\n",
    "print(set(index['index']) - set(index['query']))\n",
    "\n",
    "# (search AND query) OR (search AND retrieve)\n",
    "print(\"(search AND query) OR (search AND retrieve)\")\n",
    "print(search_and(index,'search','queri') | search_and(index,'search','retriev'))\n",
    "\n",
    "# (index OR cluster) AND (web OR system)\n",
    "print(\"(index OR cluster) AND (web OR system)\")\n",
    "print(search_or(index,'index','cluser') & search_or(index,'web','system'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
