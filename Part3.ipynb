{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "#### Inverted Index Construction:  Consider the following three short documents:\n",
    "\n",
    "Doc #1\n",
    "\n",
    "Glimpse is an indexing and query system that allows for search through a file system or document collection quickly. Glimpse is the default search engine in a larger information retrieval system. It has also been used as part of some web based search engines.\n",
    "\n",
    "Doc #2\n",
    "\n",
    "The main processes in an retrieval system are document indexing, query processing, query evaluation and relevance feedback. Among these, efficient updating of the index is critical in large scale systems.\n",
    "\n",
    "Doc #3\n",
    "\n",
    "Clusters are created from short snippets of documents retrieved by web search engines which are as good as clusters created from the full text of web documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\skhongro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\skhongro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. First remove stop words, and punctuation, and apply Porter's stemming algorithm to the 3 documents (Note: You can write your program or use the online stemming application for this purpose).  Show the resulting documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glimps', 'index', 'queri', 'system', 'allow', 'search', 'file', 'system', 'document', 'collect', 'quickli', 'glimps', 'default', 'search', 'engin', 'larger', 'inform', 'retriev', 'system', 'also', 'use', 'part', 'web', 'base', 'search', 'engin']\n",
      "\n",
      "\n",
      "['main', 'process', 'retriev', 'system', 'document', 'index', 'queri', 'process', 'queri', 'evalu', 'relev', 'feedback', 'among', 'effici', 'updat', 'index', 'critic', 'larg', 'scale', 'system']\n",
      "\n",
      "\n",
      "['cluster', 'creat', 'short', 'snippet', 'document', 'retriev', 'web', 'search', 'engin', 'good', 'cluster', 'creat', 'full', 'text', 'web', 'document']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = ['Glimpse is an indexing and query system that allows for search through a file system or document collection quickly. Glimpse is the default search engine in a larger information retrieval system. It has also been used as part of some web based search engines.'\n",
    "        ,'The main processes in an retrieval system are document indexing, query processing, query evaluation and relevance feedback. Among these, efficient updating of the index is critical in large scale systems.'\n",
    "        ,'Clusters are created from short snippets of documents retrieved by web search engines which are as good as clusters created from the full text of web documents.']\n",
    "\n",
    "for i in range(0,len(docs)):\n",
    "    docs[i] = docs[i].lower()\n",
    "\n",
    "docs_token = docs\n",
    "# docs_token[0] = word_tokenize(docs[0])\n",
    "docs_token\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "for i in range(0,3):\n",
    "    docs_token[i] = word_tokenize(docs[i])\n",
    "    docs_token[i] = [w for w in docs_token[i] if w not in stopwords.words('english')] # filter English stopwords\n",
    "    docs_token[i] = [porter.stem(tok) for tok in docs_token[i]] # apply stemmer\n",
    "    docs_token[i] = [w for w in docs_token[i] if w.isalpha()] # filter tokens that contain non-alphabetic character(s)\n",
    "\n",
    "for doc_t in docs_token:\n",
    "    print(doc_t)\n",
    "    print('\\n')\n",
    "# print (docs_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Create an inverted index of the three documents, including the dictionary and the postings. The dictionary should also contain (for each term) statistics such as total number of occurrences in the collection and the document frequency. The postings for each term should contain the document ids and the term frequencies (depict multiple postings for a term as a linked list, similar to Figure 1.3 in the IR Book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DICTIONARY\n",
      "[TOKEN: freq, docfreq]\n",
      "allow: 1, 1\n",
      "also: 1, 1\n",
      "among: 1, 1\n",
      "base: 1, 1\n",
      "cluster: 2, 1\n",
      "collect: 1, 1\n",
      "creat: 2, 1\n",
      "critic: 1, 1\n",
      "default: 1, 1\n",
      "document: 4, 3\n",
      "effici: 1, 1\n",
      "engin: 3, 2\n",
      "evalu: 1, 1\n",
      "feedback: 1, 1\n",
      "file: 1, 1\n",
      "full: 1, 1\n",
      "glimps: 2, 1\n",
      "good: 1, 1\n",
      "index: 3, 2\n",
      "inform: 1, 1\n",
      "larg: 1, 1\n",
      "larger: 1, 1\n",
      "main: 1, 1\n",
      "part: 1, 1\n",
      "process: 2, 1\n",
      "queri: 3, 2\n",
      "quickli: 1, 1\n",
      "relev: 1, 1\n",
      "retriev: 3, 3\n",
      "scale: 1, 1\n",
      "search: 4, 2\n",
      "short: 1, 1\n",
      "snippet: 1, 1\n",
      "system: 5, 2\n",
      "text: 1, 1\n",
      "updat: 1, 1\n",
      "use: 1, 1\n",
      "web: 3, 2\n",
      "\n",
      "POSTING\n",
      "[TOKEN: [docid,freq]]\n",
      "allow: [(0, 1)]\n",
      "also: [(0, 1)]\n",
      "among: [(1, 1)]\n",
      "base: [(0, 1)]\n",
      "cluster: [(2, 2)]\n",
      "collect: [(0, 1)]\n",
      "creat: [(2, 2)]\n",
      "critic: [(1, 1)]\n",
      "default: [(0, 1)]\n",
      "document: [(0, 1), (1, 1), (2, 2)]\n",
      "effici: [(1, 1)]\n",
      "engin: [(0, 2), (2, 1)]\n",
      "evalu: [(1, 1)]\n",
      "feedback: [(1, 1)]\n",
      "file: [(0, 1)]\n",
      "full: [(2, 1)]\n",
      "glimps: [(0, 2)]\n",
      "good: [(2, 1)]\n",
      "index: [(0, 1), (1, 2)]\n",
      "inform: [(0, 1)]\n",
      "larg: [(1, 1)]\n",
      "larger: [(0, 1)]\n",
      "main: [(1, 1)]\n",
      "part: [(0, 1)]\n",
      "process: [(1, 2)]\n",
      "queri: [(0, 1), (1, 2)]\n",
      "quickli: [(0, 1)]\n",
      "relev: [(1, 1)]\n",
      "retriev: [(0, 1), (1, 1), (2, 1)]\n",
      "scale: [(1, 1)]\n",
      "search: [(0, 3), (2, 1)]\n",
      "short: [(2, 1)]\n",
      "snippet: [(2, 1)]\n",
      "system: [(0, 3), (1, 2)]\n",
      "text: [(2, 1)]\n",
      "updat: [(1, 1)]\n",
      "use: [(0, 1)]\n",
      "web: [(0, 1), (2, 2)]\n"
     ]
    }
   ],
   "source": [
    "# https://spapas.github.io/2016/04/27/python-nested-list-comprehensions/\n",
    "# non_flat = [ [1,2,3], [4,5,6], [7,8] ]\n",
    "# >>> [y for x in non_flat for y in x]\n",
    "# [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# for x in non_flat:\n",
    "#     for y in x:\n",
    "#         y\n",
    "\n",
    "from collections import Counter\n",
    "word_count_dict = Counter(word for doc_t in docs_token for word in doc_t)\n",
    "word_count_dict\n",
    "\n",
    "# word_count_dict = nltk.FreqDist(word for doc_t in docs_token for word in doc_t)\n",
    "# # for key in word_count_dict.keys():\n",
    "# #     print(key+' '+str(word_count_dict[key]))\n",
    "\n",
    "# for item in sorted(word_count_dict.items()):\n",
    "#     print(item)\n",
    "\n",
    "from collections import OrderedDict\n",
    "# word_count_dict = OrderedDict()\n",
    "word_count_dict = {}\n",
    "\n",
    "for i in range(0,len(docs_token)):\n",
    "#     print(docs_token[i])\n",
    "    for word in docs_token[i]:\n",
    "        word_count_dict.setdefault(word,[]).append(i)\n",
    "\n",
    "print('DICTIONARY\\n[TOKEN: freq, docfreq]')\n",
    "# for key in word_count_dict.keys():\n",
    "for key in sorted(word_count_dict.keys()):\n",
    "    print(key+': '+\n",
    "        str(len(word_count_dict[key]))+', '+\n",
    "        str(sum(Counter(set(docid for docid in word_count_dict[key])).values()))\n",
    "    )\n",
    "#     print(Counter(docid for docid in word_count_dict[key]))\n",
    "#     print(Counter(docid for docid in word_count_dict[key]).values())\n",
    "#     print(Counter(set(docid for docid in word_count_dict[key])))\n",
    "\n",
    "print('\\nPOSTING\\n[TOKEN: [docid,freq]]')\n",
    "\n",
    "for key in sorted(word_count_dict.keys()):\n",
    "    print(key+': '+str(list(Counter(docid for docid in word_count_dict[key]).items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/28019543/inverted-index-given-a-list-of-document-tokens-using-python\n",
    "# https://codereview.stackexchange.com/questions/188918/creating-an-inverted-index-in-python\n",
    "\n",
    "# docs_token = [['glimps', 'index', 'queri', 'system', 'allow', 'search', 'file', 'system', 'document', 'collect', 'quickli', 'glimps', 'default', 'search', 'engin', 'larger', 'inform', 'retriev', 'system', 'also', 'use', 'part', 'web', 'base', 'search', 'engin']\n",
    "# ,['main', 'process', 'retriev', 'system', 'document', 'index', 'queri', 'process', 'queri', 'evalu', 'relev', 'feedback', 'among', 'effici', 'updat', 'index', 'critic', 'larg', 'scale', 'system']\n",
    "# ,['cluster', 'creat', 'short', 'snippet', 'document', 'retriev', 'web', 'search', 'engin', 'good', 'cluster', 'creat', 'full', 'text', 'web', 'document']\n",
    "# ]\n",
    "from collections import defaultdict\n",
    "index = defaultdict(list)\n",
    "\n",
    "for i in range(0,len(docs_token)):\n",
    "    for token in docs_token[i]:\n",
    "        if i not in index[token]:\n",
    "            index[token].append(i)\n",
    "            \n",
    "for key in sorted(index.keys()):\n",
    "    print(key+\": \"+str(index[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. What are the search results for the following Boolean queries (in each case explain how you obtained them from the inverted index):\n",
    "* index AND query\n",
    "* index OR query\n",
    "* index AND (NOT query)\n",
    "* (search AND query) OR (search AND retrieve)\n",
    "* (index OR cluster) AND (web OR system)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index AND query\n",
      "set()\n",
      "index AND queri\n",
      "{0, 1}\n",
      "\n",
      "index OR query\n",
      "{0, 1}\n",
      "index OR queri\n",
      "{0, 1}\n",
      "\n",
      "index AND (NOT query)\n",
      "{0, 1}\n",
      "index AND (NOT queri)\n",
      "set()\n",
      "\n",
      "(search AND query) OR (search AND retrieve)\n",
      "set()\n",
      "(search AND queri) OR (search AND retriev)\n",
      "{0, 2}\n",
      "\n",
      "(index OR cluster) AND (web OR system)\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "def search_and(index,term1,term2):\n",
    "    return set(index[term1]) & set(index[term2])\n",
    "\n",
    "def search_or(index,term1,term2):\n",
    "    return set(index[term1]) | set(index[term2])\n",
    "\n",
    "# index AND query\n",
    "print(\"index AND query\")\n",
    "print(search_and(index,'index','query'))\n",
    "print(\"index AND queri\")\n",
    "print(search_and(index,'index','queri'))\n",
    "\n",
    "# index OR query\n",
    "print(\"\\nindex OR query\")\n",
    "print(search_or(index,'index','query'))\n",
    "print(\"index OR queri\")\n",
    "print(search_or(index,'index','queri'))\n",
    "\n",
    "# index AND (NOT query)\n",
    "print(\"\\nindex AND (NOT query)\")\n",
    "print(set(index['index']) - set(index['query']))\n",
    "print(\"index AND (NOT queri)\")\n",
    "print(set(index['index']) - set(index['queri']))\n",
    "\n",
    "\n",
    "# (search AND query) OR (search AND retrieve)\n",
    "print(\"\\n(search AND query) OR (search AND retrieve)\")\n",
    "print(search_and(index,'search','query') | search_and(index,'search','retrieve'))\n",
    "print(\"(search AND queri) OR (search AND retriev)\")\n",
    "print(search_and(index,'search','queri') | search_and(index,'search','retriev'))\n",
    "\n",
    "\n",
    "# (index OR cluster) AND (web OR system)\n",
    "print(\"\\n(index OR cluster) AND (web OR system)\")\n",
    "print(search_or(index,'index','cluser') & search_or(index,'web','system'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
